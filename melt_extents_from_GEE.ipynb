{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5437ce28-f3bc-49ea-882d-72334f3f9a83",
   "metadata": {},
   "source": [
    "# SAR Melt Extent Processing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf3ed2-f031-47f9-a4a7-01c7914d5ffb",
   "metadata": {},
   "source": [
    "The notebook does the following:\n",
    "- processes the individual glaciers within the scene to get the melt extents using a \"percentile\" method:\n",
    "    - DEM elevation aggregation is done after melt extents are determined per pixel. Aggregation is done based on the glacier hypsometry.\n",
    "\n",
    "Authors: David Rounce (CMU) and Mark Fahnestock (UAF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff072da0-3eb9-4dbe-9f52-08972cacbfe8",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65277ed-79f9-4f5c-81f3-e4061aae7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and function definitions\n",
    "import glob, os, re\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2df3557-5c13-4f99-938b-fec9f0174323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5a7bed-1cf0-4c81-be5e-4b43586a9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster alternative to np.nanpercentile -- from https://krstn.eu/np.nanpercentile()-there-has-to-be-a-faster-way/\n",
    "def nan_percentile(arr, q, axis=0):\n",
    "    arr = np.asarray(arr)\n",
    "    # Move the desired axis to the front\n",
    "    arr = np.moveaxis(arr, axis, 0)\n",
    "\n",
    "    # valid (non-NaN) observations along the first axis\n",
    "    valid_obs = np.sum(np.isfinite(arr), axis=0)\n",
    "    max_val = np.nanmax(arr)\n",
    "    arr = arr.copy()  # avoid modifying original input\n",
    "    arr[np.isnan(arr)] = max_val\n",
    "    arr = np.sort(arr, axis=0)\n",
    "\n",
    "    # Handle list or single q\n",
    "    qs = [q] if np.isscalar(q) else q\n",
    "    result = []\n",
    "\n",
    "    for quant in qs:\n",
    "        k_arr = (valid_obs - 1) * (quant / 100.0)\n",
    "        f_arr = np.floor(k_arr).astype(np.int32)\n",
    "        c_arr = np.ceil(k_arr).astype(np.int32)\n",
    "        fc_equal_k_mask = f_arr == c_arr\n",
    "\n",
    "        floor_val = _zvalue_from_index(arr, f_arr) * (c_arr - k_arr)\n",
    "        ceil_val  = _zvalue_from_index(arr, c_arr) * (k_arr - f_arr)\n",
    "\n",
    "        quant_arr = floor_val + ceil_val\n",
    "        quant_arr[fc_equal_k_mask] = _zvalue_from_index(arr, k_arr.astype(np.int32))[fc_equal_k_mask]\n",
    "\n",
    "        result.append(quant_arr)\n",
    "\n",
    "    if np.isscalar(q):\n",
    "        return result[0]\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def _zvalue_from_index(arr, ind):\n",
    "    \"\"\"\n",
    "    Extracts values along the first axis of a 3D array given 2D indices.\n",
    "    \"\"\"\n",
    "    # arr shape = (depth, y, x)\n",
    "    d, nC, nR = arr.shape\n",
    "    # Compute linear indices\n",
    "    idx = nC*nR*ind + nR*np.arange(nC)[:, None] + np.arange(nR)\n",
    "    return np.take(arr, idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b4164b-1dfb-4dfb-a27a-fb0bf3db8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescStr:\n",
    "    def __init__(self):\n",
    "        self._desc = ''\n",
    "    def write(self, instr):\n",
    "        self._desc += re.sub('\\n|\\x1b.*|\\r', '', instr)\n",
    "    def read(self):\n",
    "        ret = self._desc\n",
    "        self._desc = ''\n",
    "        return ret\n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bff73-a5b9-4419-bf47-9dcc0e942430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ad6c27d-fa73-497c-9f07-a863d5e08841",
   "metadata": {},
   "source": [
    "# Find the datacube\n",
    "Currently this must be done separately for each path/row combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814b233f-2505-4745-88a6-5e6bc00d86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# set up job parameters\n",
    "# \n",
    "# Set up paths/bbox\n",
    "#out_nc = Path(\"C:/Users/jaden/Downloads/ChaltenNorth_32718_S1_cube_VH_135_1018.nc\") # netcdf output\n",
    "out_nc = Path(\"C:/Users/jaden/Downloads/Research/Notebooks/GEE\") # netcdf output\n",
    "\n",
    "\n",
    "main_directory = os.getcwd()\n",
    "fig_fp = \"C:/Users/jaden/Downloads/Research/Notebooks/Figures\"\n",
    "if not os.path.exists(fig_fp):\n",
    "    os.makedirs(fig_fp)\n",
    "csv_fp = \"C:/Users/jaden/Downloads/Research/Notebooks/csv\"\n",
    "if not os.path.exists(csv_fp):\n",
    "    os.makedirs(csv_fp)\n",
    "\n",
    "# path_frame pairs to process - dirs on same path will be searched for same-date images to be read together\n",
    "# firs key is path, which returns a list of frames for that path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bcb60c6-2de4-402c-a517-6591d01d74a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'location_str': 'Gulkana',\n",
       "  'imagedirsPath': 'D:\\\\glaciers\\\\Gulkana_Raw_SAR',\n",
       "  'scene_name': 'Gulkana',\n",
       "  'epsg_no': 32606,\n",
       "  'path_frame_dict': {'94': ['205']},\n",
       "  'Direction': 'Ascending',\n",
       "  'frame_cut': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dictionary from the JSON file\n",
    "json_path = Path(\"C:/Users/jaden/Downloads/Research/dict.json\")\n",
    "with open(json_path, \"r\") as file:\n",
    "    loaded_sar_data = json.load(file)\n",
    "\n",
    "# all processed path/frame datacubes\n",
    "loaded_sar_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e740d4-a557-4bc7-b5cd-d74f962faae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define glacier outline\n",
    "RGI_shapefile_path = \"C:/Users/jaden/Downloads/Research/Glaciers/RGI2000-v7.0-G-01_alaska/RGI2000-v7.0-G-01_alaska.shp\"\n",
    "\n",
    "xres,yres = (100.0,100.0)   # output (datacube) resolution in projection meters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371904bf-d6a5-4c6f-ba35-ca836dbcd911",
   "metadata": {},
   "source": [
    "## Select all glaciers by RGIId with sufficient coverage\n",
    "The following code is simply a function that is used to identify which glaciers are in a given scene and have coverage.  This then allows you to subset the individual glaciers once the scene is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d545512-9ac7-411c-88fe-5358dfd772d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI data\n",
    "rgi_fp = \"C:/Users/jaden/Downloads/Research/Glaciers/\" # change folderpath\n",
    "rgi_cols_drop = ['glims_id', 'anlys_id', 'subm_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41126ea-135f-4a4a-933f-5eb17a30036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectglaciersrgitable(glac_no=None, rgi_regionsO1=None, rgi_regionsO2='all', rgi_glac_number='all',\n",
    "                           rgi_fp=rgi_fp, rgi_cols_drop=rgi_cols_drop,\n",
    "                           include_landterm=True, include_laketerm=True, include_tidewater=True,\n",
    "                           glac_no_skip=None, min_glac_area_km2=0):\n",
    "    \"\"\"\n",
    "    Select all glaciers to be used in the model run according to the regions and glacier numbers defined by the RGI\n",
    "    glacier inventory. This function returns the rgi table associated with all of these glaciers.\n",
    "\n",
    "    glac_no : list of strings\n",
    "        list of strings of RGI glacier numbers (e.g., ['1.00001', '13.00001'])\n",
    "    rgi_regionsO1 : list of integers\n",
    "        list of integers of RGI order 1 regions (e.g., [1, 13])\n",
    "    rgi_regionsO2 : list of integers or 'all'\n",
    "        list of integers of RGI order 2 regions or simply 'all' for all the order 2 regions\n",
    "    rgi_glac_number : list of strings\n",
    "        list of RGI glacier numbers without the region (e.g., ['00001', '00002'])\n",
    "\n",
    "    Output: Pandas DataFrame of the glacier statistics for each glacier in the model run\n",
    "    (rows = GlacNo, columns = glacier statistics)\n",
    "    \"\"\"\n",
    "    if glac_no is not None:\n",
    "        glac_no_byregion = {}\n",
    "        rgi_regionsO1 = [int(i.split('.')[0]) for i in glac_no]\n",
    "        rgi_regionsO1 = list(set(rgi_regionsO1))\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = []\n",
    "        for i in glac_no:\n",
    "            region = i.split('.')[0]\n",
    "            glac_no_only = i.split('.')[1]\n",
    "            glac_no_byregion[int(region)].append(glac_no_only)\n",
    "\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = sorted(glac_no_byregion[region])\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    rgi_regionsO1 = sorted(rgi_regionsO1)\n",
    "    glacier_table = pd.DataFrame()\n",
    "    for region in rgi_regionsO1:\n",
    "\n",
    "        if glac_no is not None:\n",
    "            rgi_glac_number = glac_no_byregion[region]\n",
    "\n",
    "        for i in os.listdir(rgi_fp):\n",
    "            if str(region).zfill(2) in i:\n",
    "                rgi_fp_reg = rgi_fp + i + '/'\n",
    "        print(rgi_fp_reg)\n",
    "        for i in os.listdir(rgi_fp_reg):\n",
    "            if i.endswith('attributes.csv'):\n",
    "                rgi_fn = i\n",
    "                \n",
    "        try:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp_reg + rgi_fn)\n",
    "        except:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp_reg + rgi_fn, encoding='latin1')\n",
    "\n",
    "        # Populate glacer_table with the glaciers of interest\n",
    "        if rgi_regionsO2 == 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within region(s) %s are included in this model run.\" % (region))\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1\n",
    "            else:\n",
    "                glacier_table = pd.concat([glacier_table, csv_regionO1], axis=0)\n",
    "        elif rgi_regionsO2 != 'all' and rgi_glac_number == 'all':\n",
    "            print(\"All glaciers within subregion(s) %s in region %s are included in this model run.\" %\n",
    "                  (rgi_regionsO2, region))\n",
    "            for regionO2 in rgi_regionsO2:\n",
    "                regionO2_str = str(region).zfill(2) + '-' + str(regionO2).zfill(2)\n",
    "                if glacier_table.empty:\n",
    "                    glacier_table = csv_regionO1.loc[csv_regionO1['o2region'] == regionO2_str]\n",
    "                else:\n",
    "                    glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[csv_regionO1['o2region'] ==\n",
    "                                                                                regionO2_str]], axis=0))\n",
    "        else:\n",
    "            if len(rgi_glac_number) < 20:\n",
    "                print(\"%s glaciers in region %s are included: %s\" % (len(rgi_glac_number), region, rgi_glac_number))\n",
    "            else:\n",
    "                print(\"%s glaciers in region %s are included\" % (len(rgi_glac_number), region))\n",
    "                \n",
    "            rgiid_subset = ['RGI2000-v7.0-G-' + str(region).zfill(2) + '-' + x.zfill(5) for x in rgi_glac_number]\n",
    "            rgiid_all = list(csv_regionO1.rgi_id.values)\n",
    "            rgi_idx = [rgiid_all.index(x) for x in rgiid_subset if x in rgiid_all]\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1.loc[rgi_idx]\n",
    "            else:\n",
    "                glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[rgi_idx]],\n",
    "                                           axis=0))\n",
    "\n",
    "    glacier_table = glacier_table.copy()\n",
    "    # reset the index so that it is in sequential order (0, 1, 2, etc.)\n",
    "    glacier_table.reset_index(inplace=True)\n",
    "\n",
    "    # drop connectivity 2 for Greenland and Antarctica\n",
    "    glacier_table = glacier_table.loc[glacier_table['conn_lvl'].isin([0,1])]\n",
    "    glacier_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # add a simple glacier number column\n",
    "    glacier_table['glacno'] = [int(x.split('-')[-1]) for x in glacier_table.rgi_id.values]\n",
    "\n",
    "    # drop columns of data that is not being used\n",
    "    glacier_table.drop(rgi_cols_drop, axis=1, inplace=True)\n",
    "\n",
    "    # Longitude between 0-360deg (no negative)\n",
    "    glacier_table['cenlon_360'] = glacier_table['cenlon']\n",
    "    glacier_table.loc[glacier_table['cenlon'] < 0, 'cenlon_360'] = (\n",
    "            360 + glacier_table.loc[glacier_table['cenlon'] < 0, 'cenlon_360'])\n",
    "    # Subset glaciers based on their terminus type\n",
    "    termtype_values = []\n",
    "    if include_landterm:\n",
    "        termtype_values.append(0)\n",
    "        # assume dry calving, regenerated, and not assigned are land-terminating\n",
    "        termtype_values.append(3)\n",
    "        termtype_values.append(4)\n",
    "        termtype_values.append(9)\n",
    "    if include_tidewater:\n",
    "        termtype_values.append(1)\n",
    "        # assume shelf-terminating glaciers are tidewater\n",
    "        termtype_values.append(5)\n",
    "    if include_laketerm:\n",
    "        termtype_values.append(2)\n",
    "    glacier_table = glacier_table.loc[glacier_table['term_type'].isin(termtype_values)]\n",
    "    glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove glaciers below threshold\n",
    "    glacier_table = glacier_table.loc[glacier_table['area_km2'] > min_glac_area_km2,:]\n",
    "    glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove glaciers that are meant to be skipped\n",
    "    if glac_no_skip is not None:\n",
    "        glac_no_all = list(glacier_table['glacno'])\n",
    "        glac_no_unique = [x for x in glac_no_all if x not in glac_no_skip]\n",
    "        unique_idx = [glac_no_all.index(x) for x in glac_no_unique]\n",
    "        glacier_table = glacier_table.loc[unique_idx,:]\n",
    "        glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    print(\"This study is focusing on %s glaciers in region %s\" % (glacier_table.shape[0], rgi_regionsO1))\n",
    "    return glacier_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971327d-7475-44e9-bb09-8dfbaa4897db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a8da390-552c-4775-9d19-7ea18b70c86a",
   "metadata": {},
   "source": [
    "# SAR Datacube Class\n",
    "This class is used to take an existing pre-processed datacube (processed using the code above) and have the data and functions all in one place for the estimation of glacier melt extents.  Some of the functions deal with quality control, while others deal with the actual implementation of the change pixel methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21111ae8-0d07-4b71-8d22-10feac78795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sar_datacube():\n",
    "    \"\"\"\n",
    "    SAR Datacube for melt extent analyses\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ds_fn : str\n",
    "        filename of the datacube\n",
    "    scene_name : str\n",
    "        name of the scene for easier referencing and naming files\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 ds_fn=str(),\n",
    "                 scene_name=str(),\n",
    "                 rgi_reg=1,\n",
    "                 xres=None,\n",
    "                 yres=None,\n",
    "                 min_glac_area_km2=0,\n",
    "                 db_threshold=-3,\n",
    "                 db_threshold_sl=3,\n",
    "                 zscore_threshold=-2,\n",
    "                 winter_months=[1,2],\n",
    "                 snowmelt_months=[4,5,6,7],\n",
    "                 months2exclude_cp=[10, 11, 12, 1, 2],\n",
    "                 winter_std_threshold=3, # maximum winter standard deviation [dB]\n",
    "                 bin_size=20,\n",
    "                 area_bin_size=100000,\n",
    "                 allmelt_threshold=0.9,\n",
    "                 allmelt_pixels=10,\n",
    "                 nan_filter=-1e10, # value below which you can threshold for nan data\n",
    "                 min_area_frac=0.9, # minimum fraction of the total area that has data to be included\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Add attributes\n",
    "        \"\"\"\n",
    "        self.ds_fn = ds_fn\n",
    "        self.scene_name = scene_name\n",
    "        self.rgi_reg = rgi_reg\n",
    "\n",
    "        # Load xarray dataset\n",
    "        self.ds = xr.open_dataset(self.ds_fn)\n",
    "        self.data = self.ds.VH.values\n",
    "        if not nan_filter is None:\n",
    "            self.data[self.data < nan_filter] = np.nan\n",
    "        mask_good_pixels = np.sum(self.data, axis=0)\n",
    "        mask_good_pixels[~np.isnan(mask_good_pixels)] = 1\n",
    "        self.mask_good_pixels = mask_good_pixels\n",
    "        self.data_good = self.data * self.mask_good_pixels[np.newaxis,:,:]\n",
    "        \n",
    "        self.dates = self.ds.time.values\n",
    "        self.mask_values = self.ds.rgi_ind_glacier_mask.values\n",
    "        self.dem = self.ds.DEM.values\n",
    "        self.xres = xres\n",
    "        self.yres = yres\n",
    "\n",
    "        # Single Glacier Dictionary initialization\n",
    "        self.glac_bounds = {}\n",
    "        self.glac_mask = {}\n",
    "        self.glac_mask_good_pixels = {}\n",
    "        self.glac_data = {}\n",
    "        self.glac_data_cp = {}\n",
    "        self.glac_data_sl_cp = {}\n",
    "        self.min_dB_elevs = {}\n",
    "        self.glac_dem = {}\n",
    "        self.glac_bins = {}\n",
    "        self.glac_bins_center = {}\n",
    "        self.glac_area_bins = {}\n",
    "        self.glac_area_bins_center = {}\n",
    "\n",
    "        self.glac_melt_extent_elevs_percentiles = {}\n",
    "        self.glac_melt_extent_elevs_percentile_mins = {}\n",
    "        self.glac_melt_extent_elevs_percentile_maxs = {}\n",
    "        self.glac_melt_extent_areas_percentiles = {}\n",
    "        self.glac_melt_extent_areas_percentile_mins = {}\n",
    "        self.glac_melt_extent_areas_percentile_maxs = {}\n",
    "\n",
    "        self.glac_snowline_elevs_percentiles = {}\n",
    "        self.glac_snowline_elevs_percentile_mins = {}\n",
    "        self.glac_snowline_elevs_percentile_maxs = {}\n",
    "        self.glac_snowline_areas_percentiles = {}\n",
    "        self.glac_snowline_areas_percentile_mins = {}\n",
    "        self.glac_snowline_areas_percentile_maxs = {}\n",
    "        \n",
    "\n",
    "        # Attributes\n",
    "        self.min_glac_area_km2 = min_glac_area_km2\n",
    "        self.db_threshold=db_threshold\n",
    "        self.db_threshold_sl=db_threshold_sl\n",
    "        self.zscore_threshold=zscore_threshold\n",
    "        self.winter_months=winter_months\n",
    "        self.snowmelt_months=snowmelt_months\n",
    "        self.months2exclude_cp=months2exclude_cp\n",
    "        self.winter_std_threshold=winter_std_threshold\n",
    "\n",
    "        self.bin_size = bin_size\n",
    "        self.area_bin_size = area_bin_size\n",
    "        self.allmelt_threshold = allmelt_threshold,\n",
    "        self.allmelt_pixels = allmelt_pixels\n",
    "        self.min_area_frac = min_area_frac\n",
    "    \n",
    "               \n",
    "    def glacnos_to_process(self):\n",
    "        \"\"\"\n",
    "        Identify glacier numbers to process\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        main_glac_rgi_sar : pd.DataFrame\n",
    "            dataframe of relevant RGI attributes and any added ones\n",
    "        \"\"\"\n",
    "        glacnos = sorted(list(np.unique(self.mask_values)))[1:]\n",
    "        print(\"Glacnos:\")\n",
    "        print(self.mask_values)\n",
    "        glacnos_str = [str(rgi_reg) + '.' + str(x).zfill(5) for x in glacnos]\n",
    "        \n",
    "        assert len(glacnos_str) > 0, 'No glaciers to process'\n",
    "        main_glac_rgi_raw = selectglaciersrgitable(glac_no=glacnos_str, min_glac_area_km2=self.min_glac_area_km2)\n",
    "        glacnos_raw = list(main_glac_rgi_raw.glacno.values)\n",
    "        \n",
    "        # glacnos = glacnos_raw\n",
    "        \n",
    "        print(\"Initial glaciers:\", glacnos_raw)\n",
    "        print(\"Glacier count:\", len(glacnos_raw))\n",
    "        # Remove glaciers that are on the edge (and thus cut off and incomplete coverage)\n",
    "        self.mask_values = self.ds.rgi_ind_glacier_mask.values\n",
    "        glacno_edges = (list(np.unique(self.mask_values[0,:])) + \n",
    "                        list(np.unique(self.mask_values[-1,:])) + \n",
    "                        list(np.unique(self.mask_values[:,0])) + \n",
    "                        list(np.unique(self.mask_values[:,-1])))\n",
    "        glacno_edges = list(np.unique(np.array(glacno_edges)))\n",
    "        glacno_edges.remove(0)\n",
    "        glacnos = [x for x in glacnos_raw if x not in glacno_edges]\n",
    "        glac_idxs = [glacnos_raw.index(x) for x in glacnos]\n",
    "        main_glac_rgi = main_glac_rgi_raw.loc[glac_idxs]\n",
    "        main_glac_rgi.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        glacnos_2process = []\n",
    "        glacnos_dsfrac = []\n",
    "        glacnos_sarfrac = []\n",
    "        for nglac, glacno in enumerate(glacnos):\n",
    "            area_km2 = main_glac_rgi.loc[nglac,'area_km2']\n",
    "            area_ds = np.where(self.mask_values == glacno)[0].shape[0] * xres * yres / 1e6\n",
    "            area_ds_frac = area_ds / area_km2\n",
    "            \n",
    "            area_sar = np.where(~np.isnan(self.data_good[0,:,:][np.where(self.mask_values == glacno)]))[0].shape[0] * xres * yres / 1e6\n",
    "            area_sar_frac = area_sar / area_km2\n",
    "\n",
    "            #for debugging\n",
    "            print(f\"Glacier {glacno}: area_km2={area_km2:.2f}, ds_frac={area_ds_frac:.2f}, sar_frac={area_sar_frac:.2f}\")\n",
    "            \n",
    "            if area_ds_frac > self.min_area_frac and area_sar_frac > self.min_area_frac:\n",
    "                glacnos_2process.append(glacno)\n",
    "                glacnos_dsfrac.append(area_ds_frac)\n",
    "                glacnos_sarfrac.append(area_sar_frac)\n",
    "\n",
    "            \n",
    "        \n",
    "        assert len(glacnos_2process) > 0, 'No glaciers suitable for processing'\n",
    "        glacnos_2process_str = [str(rgi_reg) + '.' + str(x).zfill(5) for x in glacnos_2process]\n",
    "        main_glac_rgi_sar = selectglaciersrgitable(glac_no=glacnos_2process_str)\n",
    "        main_glac_rgi_sar['ds_area_frac'] = glacnos_dsfrac\n",
    "        # main_glac_rgi_sar['sar_area_frac'] = glacnos_sarfrac\n",
    "        main_glac_rgi_sar['rgino_str'] = [str(main_glac_rgi_sar.loc[x,'o1region']).zfill(2) + '.' + \n",
    "                                          str(main_glac_rgi_sar.loc[x,'glacno']).zfill(5) for x in np.arange(main_glac_rgi_sar.shape[0])]\n",
    "        \n",
    "        return main_glac_rgi_sar\n",
    "        \n",
    "\n",
    "    def mask_nonglacier_pixels(self, main_glac_rgi):\n",
    "        \"\"\"\n",
    "        Mask the non-glaciated pixels\n",
    "        \"\"\"\n",
    "        self.mask_values = self.ds.rgi_ind_glacier_mask.values\n",
    "\n",
    "        mask_values_minsize = np.zeros(self.mask_values.shape)\n",
    "        for glacno in main_glac_rgi.glacno.values:\n",
    "            mask_values_minsize[self.mask_values == glacno] = glacno\n",
    "        mask_values_minsize_binary = np.zeros(self.mask_values.shape)\n",
    "        mask_values_minsize_binary[mask_values_minsize>0] = 1\n",
    "        \n",
    "        data_masked = np.copy(self.data_good)\n",
    "        data_masked[self.data_good < -1e10] = np.nan # filtering out very large negative values\n",
    "        for nscene in np.arange(self.data_good.shape[0]):\n",
    "            data_scene = data_masked[nscene,:,:]\n",
    "            data_scene[mask_values_minsize==0] = np.nan\n",
    "            data_masked[nscene,:,:] = data_scene\n",
    "        \n",
    "        self.data_masked = data_masked\n",
    "          \n",
    "    def pixel_analysis(self):\n",
    "        dates_pd = pd.DatetimeIndex(self.dates)\n",
    "        self.years = [x.year for x in dates_pd]\n",
    "        self.months = [x.month for x in dates_pd]\n",
    "        self.days = [x.day for x in dates_pd]\n",
    "        self.doys = [int(x.to_julian_date() - pd.Timestamp(x.year,1,1).to_julian_date()) for x in dates_pd]\n",
    "        \n",
    "        winter_idx = [idx for idx, element in enumerate(self.months) if element in self.winter_months]\n",
    "        \n",
    "        data_winter_mean = np.nanmean(self.data_masked[winter_idx,:,:], axis=0)\n",
    "        data_winter_std = np.nanstd(self.data_masked[winter_idx,:,:], axis=0)\n",
    "        data_winter_res = self.data_masked - data_winter_mean[np.newaxis,:,:]\n",
    "        self.data_zscore = data_winter_res / data_winter_std[np.newaxis,:,:]\n",
    "\n",
    "        data_cp = np.zeros(self.data_masked.shape)\n",
    "        data_cp[np.isnan(self.data_masked)] = np.nan\n",
    "        data_cp[(data_winter_res < self.db_threshold) & (self.data_zscore < self.zscore_threshold)] = 1\n",
    "        self.data_cp = data_cp\n",
    "\n",
    "        # snowline change pixels\n",
    "        data_sl_cp = np.zeros(self.data_masked.shape)\n",
    "        data_sl_cp[np.isnan(self.data_masked)] = np.nan\n",
    "\n",
    "        # snowline change pixels -- based on minimum backscatter from each year\n",
    "        summer_idx = [idx for idx, element in enumerate(self.months) if element in self.snowmelt_months]\n",
    "        for yr in set(self.years):\n",
    "            year_idx = [idx for idx, y in enumerate(self.years) if y == yr]\n",
    "            comb_idx = list(set(year_idx).intersection(summer_idx))\n",
    "            if comb_idx: # ensure that we have any data for the year\n",
    "                # data_summer_min_yr = np.nanpercentile(self.data_masked[comb_idx,:,:], 5, axis=0) # get dB for the 5% of melt pixels\n",
    "                data_summer_min_yr = nan_percentile(self.data_masked[comb_idx,:,:], q=5, axis=0) # faster nanpercentile alternative\n",
    "                data_summer_res_yr = self.data_masked[year_idx,:,:] - data_summer_min_yr[np.newaxis,:,:]\n",
    "\n",
    "                # mask for the indices of the given year\n",
    "                for i, idx in enumerate(year_idx):\n",
    "                    mask = (data_winter_res[idx] > self.db_threshold) | (data_summer_res_yr[i] > self.db_threshold_sl)\n",
    "                    data_sl_cp[idx][mask] = 1\n",
    "        \n",
    "        # data_summer_min = np.nanpercentile(self.data_masked[summer_idx,:,:], 5, axis=0) # get dB for the 5% of melt pixels\n",
    "        # data_summer_res = self.data_masked[summer_idx,:,:] - data_summer_min[np.newaxis,:,:]\n",
    "        # data_sl_cp[(data_winter_res > self.db_threshold) | (data_summer_res > self.db_threshold_sl)] = 1\n",
    "        self.data_sl_cp = data_sl_cp\n",
    "\n",
    "\n",
    "    def annual_melt_onset_map(self):\n",
    "        self.melt_onset_doy_maps = {}\n",
    "        years_unique = np.unique(self.years)\n",
    "        for nyear, year in enumerate(years_unique):\n",
    "            # Subset dates for the given year\n",
    "            year_idx = list(np.where(np.array(self.years) == year)[0])\n",
    "            months_subset = [self.months[x] for x in year_idx]\n",
    "            doys_subset = [self.doys[x] for x in year_idx]\n",
    "        \n",
    "            # Prevent melt onset in winter months\n",
    "            data_cp_year = self.data_cp[year_idx,:,:]\n",
    "            for nmonth, month in enumerate(months_subset):\n",
    "                if month in self.months2exclude_cp:\n",
    "                    data_cp_year[nmonth,:,:] = 0\n",
    "\n",
    "            # Get the first value of 1 (i.e., first change pixel)\n",
    "            data_cp_year_onset_idx = (data_cp_year != 0).argmax(axis=0)\n",
    "            data_cp_year_onset_idx = data_cp_year_onset_idx * self.mask_good_pixels\n",
    "        \n",
    "            # Only index months where there's a value of 1\n",
    "            data_cp_year_sum = data_cp_year.sum(0)\n",
    "            data_cp_year_sum[data_cp_year_sum>0] = 1\n",
    "        \n",
    "            # Remove pixels where there is no melt\n",
    "            data_cp_year_sum[np.isnan(data_cp_year_sum)] = 0\n",
    "            data_cp_year_onset_idx[data_cp_year_sum == 0] = np.nan\n",
    "        \n",
    "            # Plot the julian day of melt onset\n",
    "            onset_idx_unique = np.unique(data_cp_year_onset_idx)\n",
    "            data_cp_year_onset_doy = np.zeros(data_cp_year_onset_idx.shape)\n",
    "            for onset_idx in onset_idx_unique:\n",
    "                if not np.isnan(onset_idx):\n",
    "                    onset_idx = int(onset_idx)\n",
    "                    doy = doys_subset[onset_idx]\n",
    "                    data_cp_year_onset_doy[data_cp_year_onset_idx == onset_idx] = doy            \n",
    "        \n",
    "            data_cp_year_onset_doy[data_cp_year_onset_doy==0] = np.nan\n",
    "\n",
    "            self.melt_onset_doy_maps[year] = data_cp_year_onset_doy\n",
    "\n",
    "\n",
    "    def single_glacier_preprocess(self, glacno=None, area_km2=10, verbose=False):\n",
    "        \"\"\"\n",
    "        Glacier melt extent elevations for individual glaciers\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glacno : int\n",
    "            glacier number within the region (SAR datacube) of interest\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.data_cp_glac : dictionary of np.arrays\n",
    "            change potential data cubes for each glacier\n",
    "        \"\"\"\n",
    "        self.glac_bounds[glacno] = {}\n",
    "        xmin = np.where(self.mask_values == glacno)[0].min() - 1\n",
    "        xmax = np.where(self.mask_values == glacno)[0].max() + 1\n",
    "        ymin = np.where(self.mask_values == glacno)[1].min() - 1\n",
    "        ymax = np.where(self.mask_values == glacno)[1].max() + 1\n",
    "        self.glac_bounds[glacno]['xmin'] = xmin\n",
    "        self.glac_bounds[glacno]['xmax'] = xmax\n",
    "        self.glac_bounds[glacno]['ymin'] = ymin\n",
    "        self.glac_bounds[glacno]['ymax'] = ymax    \n",
    "        \n",
    "        # Single Glacier Mask\n",
    "        mask_values_glac = self.mask_values[xmin:xmax+1,ymin:ymax+1]\n",
    "        mask_values_binary_nan_glac = np.copy(mask_values_glac).astype(np.float64)\n",
    "        mask_values_binary_nan_glac[mask_values_binary_nan_glac!=glacno] = np.nan\n",
    "        mask_values_binary_nan_glac[mask_values_binary_nan_glac>0] = 1\n",
    "        self.glac_mask[glacno] = mask_values_binary_nan_glac\n",
    "        \n",
    "        # Good Pixel Mask\n",
    "        self.glac_mask_good_pixels[glacno] = self.mask_good_pixels[xmin:xmax+1,ymin:ymax+1]\n",
    "        \n",
    "        # Subset data\n",
    "        self.glac_data[glacno] = self.data_masked[:,xmin:xmax+1,ymin:ymax+1] * self.glac_mask[glacno][np.newaxis,:,:]\n",
    "        self.glac_data_cp[glacno] = self.data_cp[:,xmin:xmax+1,ymin:ymax+1] * self.glac_mask[glacno][np.newaxis,:,:]\n",
    "        self.glac_data_sl_cp[glacno] = self.data_sl_cp[:,xmin:xmax+1,ymin:ymax+1] * self.glac_mask[glacno][np.newaxis,:,:]\n",
    "        \n",
    "        glac_dem = self.dem[xmin:xmax+1,ymin:ymax+1].astype(np.float64)\n",
    "        glac_dem = glac_dem * self.glac_mask[glacno] * self.glac_mask_good_pixels[glacno]\n",
    "        # glac_dem[np.isnan(self.glac_data_cp[glacno][0,:,:])] = np.nan\n",
    "        self.glac_dem[glacno] = glac_dem\n",
    "\n",
    "        # get the elevation of minimum backscatter (for defining maximum snowline cutoff)\n",
    "        glac_data = self.glac_data[glacno] * self.glac_mask[glacno] * self.glac_mask_good_pixels[glacno]\n",
    "        flat_idx = np.nanargmin(glac_data.reshape(glac_data.shape[0], -1), axis=1)\n",
    "        rows, cols = np.unravel_index(flat_idx, glac_data.shape[1:])\n",
    "        min_dB_elevs = glac_dem[rows, cols]\n",
    "        self.min_dB_elevs[glacno] = min_dB_elevs\n",
    "        \n",
    "        # equal elevation bins\n",
    "        bin_min = int(np.floor(np.nanmin(glac_dem) / bin_size) * bin_size)\n",
    "        bin_max = int(np.ceil(np.nanmax(glac_dem) / bin_size) * bin_size)\n",
    "        if verbose:\n",
    "            print('bin_min:', bin_min, '\\nbin_max:', bin_max)\n",
    "        \n",
    "        bins = np.arange(bin_min, bin_max+bin_size, bin_size)\n",
    "        bins_center = np.arange(bin_min + bin_size/2, bin_max, bin_size).astype(int)\n",
    "        bins_count, bins = np.histogram(glac_dem, bins=bins)\n",
    "        nbins = bins_center.shape[0]\n",
    "        \n",
    "        # equal area bins\n",
    "        if self.area_bin_size == 'variable': # if area bin size is variable\n",
    "            min_bins = 100 # get area based on 50 bins\n",
    "            min_bin_size = (area_km2 * 1e6) / min_bins \n",
    "            area_bin_size = np.ceil(min_bin_size / (self.xres * self.yres)) * (self.xres * self.yres)\n",
    "            area_bin_size = max(area_bin_size, 100000) # minimum of 0.1 km2 bins\n",
    "            area_bin_size = min(area_bin_size, 2e6) # maximum of 2 km2 bins\n",
    "            self.area_bin_size = area_bin_size\n",
    "    \n",
    "        assert self.area_bin_size % (self.xres * self.yres) == 0, f'`area_bin_size` is an area not compatible with DEM resolution ({self.xres} m)'\n",
    "        \n",
    "        pixels_per_area_bin = int(self.area_bin_size/(self.xres*self.yres))  # pixels per elevation bin\n",
    "        dem_sort = np.sort(glac_dem[~np.isnan(glac_dem)].flatten()) # remove NaN and sort elevation\n",
    "        \n",
    "        area_bins = dem_sort[::pixels_per_area_bin] # find elevation of bin edges\n",
    "        if area_bins[-1] != dem_sort[-1]:  # include last bin edge\n",
    "            area_bins = np.append(area_bins, dem_sort[-1])\n",
    "        area_bins_center = 0.5 * (area_bins[:-1] + area_bins[1:]) # find bin centers\n",
    "        if verbose:\n",
    "            print('area_bin_min:', area_bins[0], '\\narea_bin_max:', area_bins[-1])\n",
    "\n",
    "        self.glac_bins[glacno] = bins\n",
    "        self.glac_bins_center[glacno] = bins_center\n",
    "        self.glac_area_bins[glacno] = area_bins\n",
    "        self.glac_area_bins_center[glacno] = area_bins_center\n",
    "            \n",
    "        \n",
    "    def melt_elev_percentile_method(self, glacno, csv_fn=None, csv_sl_fn=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Compute Melt Elevations using the Percentile Method\n",
    "\n",
    "        Note: the problem with the percentile method is that ice pixels that are still melting, \n",
    "        but undetected due to the lack of snow prevent the method from using a simple pixel count.  \n",
    "        Hence, this method identifies those pixels based on an \"all melt\" threshold that identfies\n",
    "        pixels melting above them. These pixels are then assumed to be melting. \n",
    "        \n",
    "        \"All-Melt Threshold\"\n",
    "        This threshold is used to identify the elevation at which the bin is melting. \n",
    "        To avoid issues with this being applied too early (e.g., around the ELA where you may have \n",
    "        a mix of ice and firn pixels) this uses a fraction and 100%.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glacno : int\n",
    "            glacier number within the region (SAR datacube) of interest\n",
    "        verbose : Boolean\n",
    "            print some debugging information or not\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        glac_melt_extent_elevs_percentiles : dictionary of np.arrays\n",
    "            time series of the melt extent elevations for each glacier number\n",
    "        \"\"\"\n",
    "        # Sorted DEM values to use with the percentile method for easy indexing\n",
    "        dem_values_sorted = np.sort(self.glac_dem[glacno].reshape(1,-1))[0,:]\n",
    "        dem_values_sorted = dem_values_sorted[dem_values_sorted > -9999]\n",
    "        \n",
    "        # Process scenes\n",
    "        glac_dem = self.glac_dem[glacno]\n",
    "        data_cp_glac = self.glac_data_cp[glacno]\n",
    "        min_dB_elevs_glac = self.min_dB_elevs[glacno]\n",
    "        melt_extent_elevs, melt_extent_elev_mins, melt_extent_elev_maxs = [], [], []\n",
    "        melt_extent_areas, melt_extent_area_mins, melt_extent_area_maxs = [], [], []\n",
    "        snowline_elevs, snowline_elev_mins, snowline_elev_maxs = [], [], []\n",
    "        snowline_areas, snowline_area_mins, snowline_area_maxs = [], [], []\n",
    "        for nscene in np.arange(data_cp_glac.shape[0]):\n",
    "            data_cp_glac_single = data_cp_glac[nscene,:,:]\n",
    "            min_dB_elevs_glac_single = min_dB_elevs_glac[nscene]\n",
    "        \n",
    "            # Check if all nan values\n",
    "            if len(np.where(~np.isnan(data_cp_glac_single))[0]) == 0:\n",
    "                melt_extent_elevs.append(np.nan)\n",
    "                melt_extent_areas.append(np.nan)\n",
    "        \n",
    "            # Otherwise, calculate extent\n",
    "            else:\n",
    "                # ---------- ---------- ALL MELT CORRECTION: equal elevation bins ---------- ----------\n",
    "                allmelt_elev = 0\n",
    "                allmelt_100 = False\n",
    "                for nbin, bin_elev_lower in enumerate(self.glac_bins[glacno][:-1]):\n",
    "                    bin_elev_upper = self.glac_bins[glacno][nbin+1]\n",
    "                    if not allmelt_100:\n",
    "                        # Create mask based on elevations\n",
    "                        mask_bin = np.zeros(glac_dem.shape)\n",
    "                        mask_bin[(glac_dem > bin_elev_lower) & (glac_dem <= bin_elev_upper)] = 1\n",
    "                        bin_count = mask_bin.sum()\n",
    "                \n",
    "                        data_cp_bin = data_cp_glac_single * mask_bin\n",
    "                        data_cp_bin_count = np.nansum(data_cp_bin)\n",
    "                \n",
    "                        frac_melt = data_cp_bin_count / bin_count\n",
    "                \n",
    "                        # Record \"all melt\" elevation \n",
    "                        if allmelt_elev == 0 and frac_melt > allmelt_threshold and bin_count > allmelt_pixels:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                        # Record \"all melt\" elevation in the case that 100% hasn't been found yet\n",
    "                        if not allmelt_100 and frac_melt == 1:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                            allmelt_100 = True\n",
    "\n",
    "                # Apply \"all melt\" correction\n",
    "                if allmelt_elev > 0:\n",
    "                    data_cp_glac_single[glac_dem < allmelt_elev] = 1\n",
    "            \n",
    "                # ----- PERCENTILE METHOD -----\n",
    "                melt_pixels = np.nansum(data_cp_glac_single)\n",
    "        \n",
    "                # The index is associated with one less than the sum of the pixels to account for indexing starting with 0 not 1\n",
    "                if melt_pixels == 0:\n",
    "                    melt_idx = int(0)\n",
    "                else:\n",
    "                    melt_idx = int(melt_pixels - 1)\n",
    "\n",
    "                melt_extent_elev = dem_values_sorted[melt_idx]\n",
    "                melt_extent_elevs.append(melt_extent_elev)\n",
    "                \n",
    "                # percentile method uncertainty\n",
    "                nomelt_pixels_below = np.nansum((glac_dem < melt_extent_elev) & (data_cp_glac_single == 0))\n",
    "                melt_pixels_above = np.nansum((glac_dem > melt_extent_elev) & (data_cp_glac_single == 1))\n",
    "                melt_extent_elev_min = dem_values_sorted[melt_idx - nomelt_pixels_below]\n",
    "                melt_extent_elev_max = dem_values_sorted[melt_idx + melt_pixels_above]\n",
    "                melt_extent_elev_mins.append(melt_extent_elev_min)\n",
    "                melt_extent_elev_maxs.append(melt_extent_elev_max)\n",
    "\n",
    "                # ---------- ---------- ALL MELT CORRECTION: repeat for equal area elevation bin ---------- ----------\n",
    "                allmelt_elev = 0\n",
    "                allmelt_100 = False\n",
    "                for nbin, bin_elev_lower in enumerate(self.glac_area_bins[glacno][:-1]):\n",
    "                    bin_elev_upper = self.glac_area_bins[glacno][nbin+1]\n",
    "                    if not allmelt_100:\n",
    "                        # Create mask based on elevations\n",
    "                        mask_bin = np.zeros(glac_dem.shape)\n",
    "                        mask_bin[(glac_dem > bin_elev_lower) & (glac_dem <= bin_elev_upper)] = 1\n",
    "                        bin_count = mask_bin.sum()\n",
    "                \n",
    "                        data_cp_bin = data_cp_glac_single * mask_bin\n",
    "                        data_cp_bin_count = np.nansum(data_cp_bin)\n",
    "                \n",
    "                        frac_melt = data_cp_bin_count / bin_count\n",
    "                \n",
    "                        # Record \"all melt\" elevation \n",
    "                        if allmelt_elev == 0 and frac_melt > allmelt_threshold and bin_count > allmelt_pixels:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                        # Record \"all melt\" elevation in the case that 100% hasn't been found yet\n",
    "                        if not allmelt_100 and frac_melt == 1:\n",
    "                            allmelt_elev = bin_elev_lower\n",
    "                            allmelt_100 = True\n",
    "\n",
    "                # Apply \"all melt\" correction\n",
    "                if allmelt_elev > 0:\n",
    "                    data_cp_glac_single[glac_dem < allmelt_elev] = 1\n",
    "            \n",
    "                # ----- PERCENTILE METHOD -----\n",
    "                melt_pixels = np.nansum(data_cp_glac_single)\n",
    "        \n",
    "                # The index is associated with one less than the sum of the pixels to account for indexing starting with 0 not 1\n",
    "                if melt_pixels == 0:\n",
    "                    melt_idx = int(0)\n",
    "                else:\n",
    "                    melt_idx = int(melt_pixels - 1)               \n",
    "                melt_extent_area = melt_idx*self.xres*self.yres\n",
    "                melt_extent_areas.append(melt_extent_area)\n",
    "\n",
    "                # percentile method uncertainty\n",
    "                nomelt_pixels_below = np.nansum((glac_dem < dem_values_sorted[melt_idx]) & (data_cp_glac_single == 0))\n",
    "                melt_pixels_above = np.nansum((glac_dem > dem_values_sorted[melt_idx]) & (data_cp_glac_single == 1))\n",
    "                melt_extent_area_min = (melt_idx - nomelt_pixels_below)*self.xres*self.yres\n",
    "                melt_extent_area_max = (melt_idx + melt_pixels_above)*self.xres*self.yres\n",
    "                melt_extent_area_mins.append(melt_extent_area_min)\n",
    "                melt_extent_area_maxs.append(melt_extent_area_max)\n",
    "\n",
    "                # ------------------------- SNOWLINES ------------------------------\n",
    "                # SNOWLINES: cp values that are back to 0 below the melt extent (minimum to be conservative) or elevation of min backscatter\n",
    "                data_sl_cp_glac = self.glac_data_sl_cp[glacno]\n",
    "                data_sl_cp_glac_single = data_sl_cp_glac[nscene,:,:]\n",
    "                data_sl_elev_max = min(min_dB_elevs_glac_single, melt_extent_elev_min)\n",
    "\n",
    "                snowline_cp = (data_sl_cp_glac_single == 1) & (glac_dem < data_sl_elev_max)\n",
    "                snowline_idx = max(np.nansum(snowline_cp) - 1, 0)\n",
    "                \n",
    "                snowline_elev = dem_values_sorted[snowline_idx]\n",
    "                snowline_area = snowline_idx*self.xres*self.yres\n",
    "\n",
    "                # percentile method uncertainty for snowlines\n",
    "                # snow_pixels_below = np.nansum((glac_dem < snowline_elev) & (data_sl_cp_glac_single == 0))\n",
    "                # nosnow_pixels_above = np.nansum((glac_dem > snowline_elev) & (data_sl_cp_glac_single == 1) & \n",
    "                #                                 (glac_dem < melt_extent_elev_min))\n",
    "                snow_pixels_below = (np.nansum((glac_dem < snowline_elev) & (data_sl_cp_glac_single == 0)) + \n",
    "                                     np.nansum((glac_dem < snowline_elev) & self.glac_mask[glacno].astype(bool) & ~self.glac_mask_good_pixels[glacno].astype(bool)))\n",
    "                nosnow_pixels_above = (np.nansum((glac_dem > snowline_elev) & (data_sl_cp_glac_single == 1) & (glac_dem < melt_extent_elev_min)) +\n",
    "                                       np.nansum((glac_dem > snowline_elev) & self.glac_mask[glacno].astype(bool) & ~self.glac_mask_good_pixels[glacno].astype(bool)))\n",
    "\n",
    "                snowline_elev_min = dem_values_sorted[snowline_idx - snow_pixels_below]\n",
    "                snowline_elev_max = dem_values_sorted[snowline_idx + nosnow_pixels_above]\n",
    "                snowline_area_min = (snowline_idx - snow_pixels_below)*self.xres*self.yres\n",
    "                snowline_area_max = (snowline_idx + nosnow_pixels_above)*self.xres*self.yres\n",
    "\n",
    "                # add to lists\n",
    "                snowline_elevs.append(snowline_elev)\n",
    "                snowline_areas.append(snowline_area)\n",
    "                snowline_elev_mins.append(snowline_elev_min)\n",
    "                snowline_elev_maxs.append(snowline_elev_max)\n",
    "                snowline_area_mins.append(snowline_area_min)\n",
    "                snowline_area_maxs.append(snowline_area_max)\n",
    "\n",
    "        \n",
    "        self.glac_melt_extent_elevs_percentiles[glacno] = np.array(melt_extent_elevs)\n",
    "        self.glac_melt_extent_elevs_percentile_mins[glacno] = np.array(melt_extent_elev_mins)\n",
    "        self.glac_melt_extent_elevs_percentile_maxs[glacno] = np.array(melt_extent_elev_maxs)\n",
    "        self.glac_melt_extent_areas_percentiles[glacno] = np.array(melt_extent_areas)\n",
    "        self.glac_melt_extent_areas_percentile_mins[glacno] = np.array(melt_extent_area_mins)\n",
    "        self.glac_melt_extent_areas_percentile_maxs[glacno] = np.array(melt_extent_area_maxs)\n",
    "\n",
    "        self.glac_snowline_elevs_percentiles[glacno] = np.array(snowline_elevs)\n",
    "        self.glac_snowline_elevs_percentile_mins[glacno] = np.array(snowline_elev_mins)\n",
    "        self.glac_snowline_elevs_percentile_maxs[glacno] = np.array(snowline_elev_maxs)\n",
    "        self.glac_snowline_areas_percentiles[glacno] = np.array(snowline_areas)\n",
    "        self.glac_snowline_areas_percentile_mins[glacno] = np.array(snowline_area_mins)\n",
    "        self.glac_snowline_areas_percentile_maxs[glacno] = np.array(snowline_area_maxs)\n",
    "        \n",
    "        # Export binned data        \n",
    "        if not csv_fn is None:\n",
    "            me_df = pd.DataFrame(self.glac_melt_extent_elevs_percentiles[glacno], columns=['melt_extent_elev_m'], index=self.dates)\n",
    "            me_df['melt_extent_elev_min_m'] = self.glac_melt_extent_elevs_percentile_mins[glacno]\n",
    "            me_df['melt_extent_elev_max_m'] = self.glac_melt_extent_elevs_percentile_maxs[glacno]\n",
    "            me_df['melt_extent_elev_diff_mean_m'] = ((me_df['melt_extent_elev_max_m'] - me_df['melt_extent_elev_m']) +\n",
    "                                                     (me_df['melt_extent_elev_m'] - me_df['melt_extent_elev_min_m']))/2\n",
    "            me_df.to_csv(csv_fn)\n",
    "            \n",
    "            me_df = pd.DataFrame(self.glac_melt_extent_areas_percentiles[glacno], columns=['melt_extent_area_m2'], index=self.dates)\n",
    "            me_df['melt_extent_area_min_m2'] = self.glac_melt_extent_areas_percentile_mins[glacno]\n",
    "            me_df['melt_extent_area_max_m2'] = self.glac_melt_extent_areas_percentile_maxs[glacno]\n",
    "            me_df['melt_extent_area_diff_mean_m2'] = ((me_df['melt_extent_area_max_m2'] - me_df['melt_extent_area_m2']) +\n",
    "                                                      (me_df['melt_extent_area_m2'] - me_df['melt_extent_area_min_m2']))/2\n",
    "            me_df.to_csv(csv_fn[:-4]+'_eabin.csv')\n",
    "        else:\n",
    "            return (self.dates, self.glac_melt_extent_elevs_percentiles[glacno], self.glac_melt_extent_areas_percentiles[glacno], \n",
    "                    self.glac_snowline_elevs_percentiles[glacno], self.glac_snowline_areas_percentiles[glacno])\n",
    "\n",
    "        if not csv_sl_fn is None:\n",
    "            sl_df = pd.DataFrame(self.glac_snowline_elevs_percentiles[glacno], columns=['snowline_elev_m'], index=self.dates)\n",
    "            sl_df['snowline_elev_min_m'] = self.glac_snowline_elevs_percentile_mins[glacno]\n",
    "            sl_df['snowline_elev_max_m'] = self.glac_snowline_elevs_percentile_maxs[glacno]\n",
    "            sl_df['snowline_elev_diff_mean_m'] = ((sl_df['snowline_elev_max_m'] - sl_df['snowline_elev_m']) +\n",
    "                                                  (sl_df['snowline_elev_m'] - sl_df['snowline_elev_min_m']))/2\n",
    "            sl_df.to_csv(csv_sl_fn)\n",
    "            \n",
    "            sl_df = pd.DataFrame(self.glac_snowline_areas_percentiles[glacno], columns=['snowline_area_m2'], index=self.dates)\n",
    "            sl_df['snowline_area_min_m2'] = self.glac_snowline_areas_percentile_mins[glacno]\n",
    "            sl_df['snowline_area_max_m2'] = self.glac_snowline_areas_percentile_maxs[glacno]\n",
    "            sl_df['snowline_area_diff_mean_m2'] = ((sl_df['snowline_area_max_m2'] - sl_df['snowline_area_m2']) +\n",
    "                                                   (sl_df['snowline_area_m2'] - sl_df['snowline_area_min_m2']))/2\n",
    "            sl_df.to_csv(csv_sl_fn[:-4]+'_eabin.csv')\n",
    "\n",
    "    \n",
    "    def db_heatmap(self, glacno, csv_fn=None, hyps_fn=None):\n",
    "        \"\"\"\n",
    "        Bin backscatter to produce heatmaps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glacno : int\n",
    "            glacier number within the region (SAR datacube) of interest\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        glac_binned_db : dictionary of np.arrays\n",
    "            binned backscatter for each glacier number\n",
    "        \"\"\"\n",
    "        for i in [0, 1]:\n",
    "            if i == 0:\n",
    "                bins = self.glac_bins[glacno]\n",
    "                bins_center = self.glac_bins_center[glacno]\n",
    "            else:\n",
    "                bins = self.glac_area_bins[glacno]\n",
    "                bins_center = self.glac_area_bins_center[glacno]\n",
    "            \n",
    "            glac_dem = self.glac_dem[glacno]\n",
    "            glac_data = self.glac_data[glacno]\n",
    "            \n",
    "            db_bin = np.zeros((len(bins)-1, glac_data.shape[0]))\n",
    "            db_bin[:,:] = np.nan\n",
    "            binned_pixels = np.zeros((len(bins)-1))\n",
    "            for nbin, bin_elev_lower in enumerate(bins[:-1]):\n",
    "                # Create mask based on elevations\n",
    "                bin_elev_upper = bins[nbin+1]\n",
    "                mask_bin = np.zeros(glac_dem.shape)\n",
    "                mask_bin[(glac_dem > bin_elev_lower) & (glac_dem <= bin_elev_upper)] = 1\n",
    "                bin_pixels = mask_bin.sum()\n",
    "            \n",
    "                # Mask pixels\n",
    "                data_glac_singlebin = glac_data * mask_bin[np.newaxis,:,:]\n",
    "            \n",
    "                # Manually average based on summing and pixel counts\n",
    "                #  note: this avoids masking the entire data stack which is slow\n",
    "                bin_sar_series = np.nansum(data_glac_singlebin, axis=(1, 2)) / bin_pixels\n",
    "                bin_sar_series[bin_sar_series == 0] = np.nan\n",
    "                db_bin[nbin,:] = bin_sar_series\n",
    "            \n",
    "                binned_pixels[nbin] = bin_pixels\n",
    "    \n",
    "            # Export binned data\n",
    "            if not csv_fn is None:\n",
    "                db_bin_df = pd.DataFrame(db_bin, columns=self.dates, index=bins_center)\n",
    "                if i == 0:\n",
    "                    db_bin_df.to_csv(csv_fn)\n",
    "                else:\n",
    "                    db_bin_df.to_csv(csv_fn[:-4]+'_eabin.csv')\n",
    "\n",
    "            # Export hypsometry\n",
    "            binned_area = binned_pixels * self.xres * self.yres\n",
    "            if not hyps_fn is None:\n",
    "                hyps_df = pd.DataFrame(binned_area, columns=['area_m2'], index=bins_center)\n",
    "                hyps_df.index.name = 'Elev_m'\n",
    "                if i == 0:\n",
    "                    hyps_df.to_csv(hyps_fn)\n",
    "                else:\n",
    "                    hyps_df.to_csv(hyps_fn[:-4]+'_eabin.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cecd22-5db9-4cc1-8cfb-e33de2e382d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7d0d690-ec5d-4fb4-9686-88740136e194",
   "metadata": {},
   "source": [
    "Main function to process data and extract melt extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f47dbc2-0951-4f31-a6ce-7d71cf2300cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/jaden/Downloads/Research/csv\"  # replace with your folder path\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77be5b6-1fd0-4f35-8026-030132804379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171b3470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(\n",
       "    --jp-content-font-color0,\n",
       "    var(--pst-color-text-base rgba(0, 0, 0, 1))\n",
       "  );\n",
       "  --xr-font-color2: var(\n",
       "    --jp-content-font-color2,\n",
       "    var(--pst-color-text-base, rgba(0, 0, 0, 0.54))\n",
       "  );\n",
       "  --xr-font-color3: var(\n",
       "    --jp-content-font-color3,\n",
       "    var(--pst-color-text-base, rgba(0, 0, 0, 0.38))\n",
       "  );\n",
       "  --xr-border-color: var(\n",
       "    --jp-border-color2,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 10))\n",
       "  );\n",
       "  --xr-disabled-color: var(\n",
       "    --jp-layout-color3,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 40))\n",
       "  );\n",
       "  --xr-background-color: var(\n",
       "    --jp-layout-color0,\n",
       "    var(--pst-color-on-background, white)\n",
       "  );\n",
       "  --xr-background-color-row-even: var(\n",
       "    --jp-layout-color1,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 5))\n",
       "  );\n",
       "  --xr-background-color-row-odd: var(\n",
       "    --jp-layout-color2,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 15))\n",
       "  );\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "html[data-theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: var(\n",
       "    --jp-content-font-color0,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 1))\n",
       "  );\n",
       "  --xr-font-color2: var(\n",
       "    --jp-content-font-color2,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 0.54))\n",
       "  );\n",
       "  --xr-font-color3: var(\n",
       "    --jp-content-font-color3,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 0.38))\n",
       "  );\n",
       "  --xr-border-color: var(\n",
       "    --jp-border-color2,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 10))\n",
       "  );\n",
       "  --xr-disabled-color: var(\n",
       "    --jp-layout-color3,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 40))\n",
       "  );\n",
       "  --xr-background-color: var(\n",
       "    --jp-layout-color0,\n",
       "    var(--pst-color-on-background, #111111)\n",
       "  );\n",
       "  --xr-background-color-row-even: var(\n",
       "    --jp-layout-color1,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 5))\n",
       "  );\n",
       "  --xr-background-color-row-odd: var(\n",
       "    --jp-layout-color2,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 15))\n",
       "  );\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "  height: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "  border: 2px solid transparent !important;\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0) !important;\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: \"►\";\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: \"▼\";\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: \"(\";\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: \")\";\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: \",\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  border-color: var(--xr-background-color-row-odd);\n",
       "  margin-bottom: 0;\n",
       "  padding-top: 2px;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "  border-color: var(--xr-background-color-row-even);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  border-top: 2px dotted var(--xr-background-color);\n",
       "  padding-bottom: 20px !important;\n",
       "  padding-top: 10px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in + label,\n",
       ".xr-var-data-in + label,\n",
       ".xr-index-data-in + label {\n",
       "  padding: 0 1px;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-data > pre,\n",
       ".xr-index-data > pre,\n",
       ".xr-var-data > table > tbody > tr {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked + label > .xr-icon-file-text2,\n",
       ".xr-var-data-in:checked + label > .xr-icon-database,\n",
       ".xr-index-data-in:checked + label > .xr-icon-database {\n",
       "  color: var(--xr-font-color0);\n",
       "  filter: drop-shadow(1px 1px 5px var(--xr-font-color2));\n",
       "  stroke-width: 0.8px;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 2kB\n",
       "Dimensions:               (time: 79, lon: 1, lat: 1)\n",
       "Coordinates:\n",
       "  * time                  (time) datetime64[ns] 632B 2017-06-04T15:56:08 ... ...\n",
       "  * lon                   (lon) float64 8B -130.7\n",
       "  * lat                   (lat) float64 8B 75.64\n",
       "Data variables:\n",
       "    VH                    (time, lon, lat) float32 316B ...\n",
       "    DEM                   (time, lon, lat) float32 316B ...\n",
       "    rgi_ind_glacier_mask  (time, lon, lat) float32 316B ...\n",
       "Attributes:\n",
       "    crs:      EPSG:4326</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-78db72f1-a5b7-4db6-a4ba-063bd98ac9b4' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-78db72f1-a5b7-4db6-a4ba-063bd98ac9b4' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 79</li><li><span class='xr-has-index'>lon</span>: 1</li><li><span class='xr-has-index'>lat</span>: 1</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-6798989d-4a63-44f1-b267-8b8721d2a8cf' class='xr-section-summary-in' type='checkbox'  checked><label for='section-6798989d-4a63-44f1-b267-8b8721d2a8cf' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2017-06-04T15:56:08 ... 2019-12-...</div><input id='attrs-eee09747-fa9f-47ff-a2ad-22eb5c5ad94f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-eee09747-fa9f-47ff-a2ad-22eb5c5ad94f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-87d6890c-1012-42ee-a843-be9678e00111' class='xr-var-data-in' type='checkbox'><label for='data-87d6890c-1012-42ee-a843-be9678e00111' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2017-06-04T15:56:08.000000000&#x27;, &#x27;2017-06-16T15:56:09.000000000&#x27;,\n",
       "       &#x27;2017-06-28T15:56:09.000000000&#x27;, &#x27;2017-07-10T15:56:10.000000000&#x27;,\n",
       "       &#x27;2017-07-22T15:56:11.000000000&#x27;, &#x27;2017-08-03T15:56:12.000000000&#x27;,\n",
       "       &#x27;2017-08-15T15:56:12.000000000&#x27;, &#x27;2017-08-27T15:56:13.000000000&#x27;,\n",
       "       &#x27;2017-09-08T15:56:13.000000000&#x27;, &#x27;2017-09-20T15:56:14.000000000&#x27;,\n",
       "       &#x27;2017-10-02T15:56:14.000000000&#x27;, &#x27;2017-10-14T15:56:14.000000000&#x27;,\n",
       "       &#x27;2017-10-26T15:56:14.000000000&#x27;, &#x27;2017-11-07T15:56:14.000000000&#x27;,\n",
       "       &#x27;2017-11-19T15:56:14.000000000&#x27;, &#x27;2017-12-01T15:56:13.000000000&#x27;,\n",
       "       &#x27;2017-12-13T15:56:13.000000000&#x27;, &#x27;2017-12-25T15:56:12.000000000&#x27;,\n",
       "       &#x27;2018-01-06T15:56:12.000000000&#x27;, &#x27;2018-01-18T15:56:11.000000000&#x27;,\n",
       "       &#x27;2018-01-30T15:56:11.000000000&#x27;, &#x27;2018-02-11T15:56:11.000000000&#x27;,\n",
       "       &#x27;2018-02-23T15:56:11.000000000&#x27;, &#x27;2018-03-07T15:56:11.000000000&#x27;,\n",
       "       &#x27;2018-03-19T15:56:11.000000000&#x27;, &#x27;2018-03-31T15:56:11.000000000&#x27;,\n",
       "       &#x27;2018-04-12T15:56:12.000000000&#x27;, &#x27;2018-04-24T15:56:12.000000000&#x27;,\n",
       "       &#x27;2018-05-06T15:56:13.000000000&#x27;, &#x27;2018-05-18T15:56:13.000000000&#x27;,\n",
       "       &#x27;2018-05-30T15:56:14.000000000&#x27;, &#x27;2018-06-11T15:56:15.000000000&#x27;,\n",
       "       &#x27;2018-06-23T15:56:16.000000000&#x27;, &#x27;2018-07-05T15:56:16.000000000&#x27;,\n",
       "       &#x27;2018-07-17T15:56:17.000000000&#x27;, &#x27;2018-07-29T15:56:18.000000000&#x27;,\n",
       "       &#x27;2018-08-10T15:56:18.000000000&#x27;, &#x27;2018-08-22T15:56:19.000000000&#x27;,\n",
       "       &#x27;2018-09-03T15:56:20.000000000&#x27;, &#x27;2018-09-15T15:56:20.000000000&#x27;,\n",
       "       &#x27;2018-09-27T15:56:20.000000000&#x27;, &#x27;2018-10-09T15:56:21.000000000&#x27;,\n",
       "       &#x27;2018-10-21T15:56:21.000000000&#x27;, &#x27;2018-11-02T15:56:21.000000000&#x27;,\n",
       "       &#x27;2018-11-14T15:56:20.000000000&#x27;, &#x27;2018-11-26T15:56:20.000000000&#x27;,\n",
       "       &#x27;2018-12-08T15:56:20.000000000&#x27;, &#x27;2018-12-20T15:56:19.000000000&#x27;,\n",
       "       &#x27;2019-01-01T15:56:19.000000000&#x27;, &#x27;2019-01-13T15:56:18.000000000&#x27;,\n",
       "       &#x27;2019-01-25T15:56:18.000000000&#x27;, &#x27;2019-02-06T15:56:17.000000000&#x27;,\n",
       "       &#x27;2019-02-18T15:56:17.000000000&#x27;, &#x27;2019-03-02T15:56:17.000000000&#x27;,\n",
       "       &#x27;2019-03-14T15:56:17.000000000&#x27;, &#x27;2019-03-26T15:56:18.000000000&#x27;,\n",
       "       &#x27;2019-04-07T15:56:18.000000000&#x27;, &#x27;2019-04-19T15:56:18.000000000&#x27;,\n",
       "       &#x27;2019-05-01T15:56:19.000000000&#x27;, &#x27;2019-05-13T15:56:19.000000000&#x27;,\n",
       "       &#x27;2019-05-25T15:56:20.000000000&#x27;, &#x27;2019-06-06T15:56:20.000000000&#x27;,\n",
       "       &#x27;2019-06-18T15:56:21.000000000&#x27;, &#x27;2019-06-30T15:56:22.000000000&#x27;,\n",
       "       &#x27;2019-07-12T15:56:23.000000000&#x27;, &#x27;2019-07-24T15:56:23.000000000&#x27;,\n",
       "       &#x27;2019-08-05T15:56:24.000000000&#x27;, &#x27;2019-08-17T15:56:25.000000000&#x27;,\n",
       "       &#x27;2019-08-29T15:56:25.000000000&#x27;, &#x27;2019-09-10T15:56:26.000000000&#x27;,\n",
       "       &#x27;2019-09-22T15:56:27.000000000&#x27;, &#x27;2019-10-04T15:56:27.000000000&#x27;,\n",
       "       &#x27;2019-10-16T15:56:27.000000000&#x27;, &#x27;2019-10-28T15:56:27.000000000&#x27;,\n",
       "       &#x27;2019-11-09T15:56:27.000000000&#x27;, &#x27;2019-11-21T15:56:27.000000000&#x27;,\n",
       "       &#x27;2019-12-03T15:56:26.000000000&#x27;, &#x27;2019-12-15T15:56:26.000000000&#x27;,\n",
       "       &#x27;2019-12-27T15:56:25.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-130.7</div><input id='attrs-c911362d-56cb-4252-9024-8ed166d63177' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c911362d-56cb-4252-9024-8ed166d63177' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-45c3540c-67e5-46c2-af0d-f8db57870022' class='xr-var-data-in' type='checkbox'><label for='data-45c3540c-67e5-46c2-af0d-f8db57870022' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-130.711538])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>75.64</div><input id='attrs-c056250e-1f3d-4ba1-871a-f505690d08e1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c056250e-1f3d-4ba1-871a-f505690d08e1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f8201d3e-4786-4c93-8a43-86ef6a559c4c' class='xr-var-data-in' type='checkbox'><label for='data-f8201d3e-4786-4c93-8a43-86ef6a559c4c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([75.641355])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-ae4ca04d-144c-4eb5-8f5b-e524e4701429' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ae4ca04d-144c-4eb5-8f5b-e524e4701429' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>VH</span></div><div class='xr-var-dims'>(time, lon, lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-c7fe4d7c-1f74-4184-9818-60110b349020' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c7fe4d7c-1f74-4184-9818-60110b349020' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-26c0fde7-e1c6-4124-a14a-e56d73539009' class='xr-var-data-in' type='checkbox'><label for='data-26c0fde7-e1c6-4124-a14a-e56d73539009' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>id :</span></dt><dd>VH</dd><dt><span>data_type :</span></dt><dd>{&#x27;type&#x27;: &#x27;PixelType&#x27;, &#x27;precision&#x27;: &#x27;float&#x27;}</dd><dt><span>dimensions :</span></dt><dd>[29048 22165]</dd><dt><span>crs :</span></dt><dd>EPSG:4326</dd><dt><span>crs_transform :</span></dt><dd>[ 1.00000000e+01  0.00000000e+00  2.39705383e+05  0.00000000e+00\n",
       " -1.00000000e+01  6.94424174e+06]</dd><dt><span>bounds :</span></dt><dd>[-145.71153808   60.64135503 -140.43007714   62.54565471]</dd></dl></div><div class='xr-var-data'><pre>[79 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>DEM</span></div><div class='xr-var-dims'>(time, lon, lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b95ef4a0-f791-49a4-b83e-7b5bfcc2ab5e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b95ef4a0-f791-49a4-b83e-7b5bfcc2ab5e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b5cf48f1-dd70-42d3-8b31-d94c06dbf0ae' class='xr-var-data-in' type='checkbox'><label for='data-b5cf48f1-dd70-42d3-8b31-d94c06dbf0ae' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>id :</span></dt><dd>DEM</dd><dt><span>data_type :</span></dt><dd>{&#x27;type&#x27;: &#x27;PixelType&#x27;, &#x27;precision&#x27;: &#x27;float&#x27;}</dd><dt><span>crs :</span></dt><dd>EPSG:4326</dd><dt><span>crs_transform :</span></dt><dd>[1 0 0 0 1 0]</dd><dt><span>bounds :</span></dt><dd>[-145.71153808   60.64135503 -140.43007714   62.54565471]</dd></dl></div><div class='xr-var-data'><pre>[79 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>rgi_ind_glacier_mask</span></div><div class='xr-var-dims'>(time, lon, lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-546e1456-caaa-4093-9d61-06c72bd85b64' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-546e1456-caaa-4093-9d61-06c72bd85b64' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ca7ec688-c668-4982-876e-cb7c65611b84' class='xr-var-data-in' type='checkbox'><label for='data-ca7ec688-c668-4982-876e-cb7c65611b84' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>id :</span></dt><dd>rgi_ind_glacier_mask</dd><dt><span>data_type :</span></dt><dd>{&#x27;type&#x27;: &#x27;PixelType&#x27;, &#x27;precision&#x27;: &#x27;double&#x27;}</dd><dt><span>crs :</span></dt><dd>EPSG:4326</dd><dt><span>crs_transform :</span></dt><dd>[1 0 0 0 1 0]</dd><dt><span>bounds :</span></dt><dd>[-145.71153808   60.64135503 -140.43007714   62.54565471]</dd></dl></div><div class='xr-var-data'><pre>[79 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b10cacf8-aa11-4cb5-858b-a4c51e5f4907' class='xr-section-summary-in' type='checkbox'  ><label for='section-b10cacf8-aa11-4cb5-858b-a4c51e5f4907' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-e73f5c32-974d-4d00-818a-62e79d81912b' class='xr-index-data-in' type='checkbox'/><label for='index-e73f5c32-974d-4d00-818a-62e79d81912b' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2017-06-04 15:56:08&#x27;, &#x27;2017-06-16 15:56:09&#x27;,\n",
       "               &#x27;2017-06-28 15:56:09&#x27;, &#x27;2017-07-10 15:56:10&#x27;,\n",
       "               &#x27;2017-07-22 15:56:11&#x27;, &#x27;2017-08-03 15:56:12&#x27;,\n",
       "               &#x27;2017-08-15 15:56:12&#x27;, &#x27;2017-08-27 15:56:13&#x27;,\n",
       "               &#x27;2017-09-08 15:56:13&#x27;, &#x27;2017-09-20 15:56:14&#x27;,\n",
       "               &#x27;2017-10-02 15:56:14&#x27;, &#x27;2017-10-14 15:56:14&#x27;,\n",
       "               &#x27;2017-10-26 15:56:14&#x27;, &#x27;2017-11-07 15:56:14&#x27;,\n",
       "               &#x27;2017-11-19 15:56:14&#x27;, &#x27;2017-12-01 15:56:13&#x27;,\n",
       "               &#x27;2017-12-13 15:56:13&#x27;, &#x27;2017-12-25 15:56:12&#x27;,\n",
       "               &#x27;2018-01-06 15:56:12&#x27;, &#x27;2018-01-18 15:56:11&#x27;,\n",
       "               &#x27;2018-01-30 15:56:11&#x27;, &#x27;2018-02-11 15:56:11&#x27;,\n",
       "               &#x27;2018-02-23 15:56:11&#x27;, &#x27;2018-03-07 15:56:11&#x27;,\n",
       "               &#x27;2018-03-19 15:56:11&#x27;, &#x27;2018-03-31 15:56:11&#x27;,\n",
       "               &#x27;2018-04-12 15:56:12&#x27;, &#x27;2018-04-24 15:56:12&#x27;,\n",
       "               &#x27;2018-05-06 15:56:13&#x27;, &#x27;2018-05-18 15:56:13&#x27;,\n",
       "               &#x27;2018-05-30 15:56:14&#x27;, &#x27;2018-06-11 15:56:15&#x27;,\n",
       "               &#x27;2018-06-23 15:56:16&#x27;, &#x27;2018-07-05 15:56:16&#x27;,\n",
       "               &#x27;2018-07-17 15:56:17&#x27;, &#x27;2018-07-29 15:56:18&#x27;,\n",
       "               &#x27;2018-08-10 15:56:18&#x27;, &#x27;2018-08-22 15:56:19&#x27;,\n",
       "               &#x27;2018-09-03 15:56:20&#x27;, &#x27;2018-09-15 15:56:20&#x27;,\n",
       "               &#x27;2018-09-27 15:56:20&#x27;, &#x27;2018-10-09 15:56:21&#x27;,\n",
       "               &#x27;2018-10-21 15:56:21&#x27;, &#x27;2018-11-02 15:56:21&#x27;,\n",
       "               &#x27;2018-11-14 15:56:20&#x27;, &#x27;2018-11-26 15:56:20&#x27;,\n",
       "               &#x27;2018-12-08 15:56:20&#x27;, &#x27;2018-12-20 15:56:19&#x27;,\n",
       "               &#x27;2019-01-01 15:56:19&#x27;, &#x27;2019-01-13 15:56:18&#x27;,\n",
       "               &#x27;2019-01-25 15:56:18&#x27;, &#x27;2019-02-06 15:56:17&#x27;,\n",
       "               &#x27;2019-02-18 15:56:17&#x27;, &#x27;2019-03-02 15:56:17&#x27;,\n",
       "               &#x27;2019-03-14 15:56:17&#x27;, &#x27;2019-03-26 15:56:18&#x27;,\n",
       "               &#x27;2019-04-07 15:56:18&#x27;, &#x27;2019-04-19 15:56:18&#x27;,\n",
       "               &#x27;2019-05-01 15:56:19&#x27;, &#x27;2019-05-13 15:56:19&#x27;,\n",
       "               &#x27;2019-05-25 15:56:20&#x27;, &#x27;2019-06-06 15:56:20&#x27;,\n",
       "               &#x27;2019-06-18 15:56:21&#x27;, &#x27;2019-06-30 15:56:22&#x27;,\n",
       "               &#x27;2019-07-12 15:56:23&#x27;, &#x27;2019-07-24 15:56:23&#x27;,\n",
       "               &#x27;2019-08-05 15:56:24&#x27;, &#x27;2019-08-17 15:56:25&#x27;,\n",
       "               &#x27;2019-08-29 15:56:25&#x27;, &#x27;2019-09-10 15:56:26&#x27;,\n",
       "               &#x27;2019-09-22 15:56:27&#x27;, &#x27;2019-10-04 15:56:27&#x27;,\n",
       "               &#x27;2019-10-16 15:56:27&#x27;, &#x27;2019-10-28 15:56:27&#x27;,\n",
       "               &#x27;2019-11-09 15:56:27&#x27;, &#x27;2019-11-21 15:56:27&#x27;,\n",
       "               &#x27;2019-12-03 15:56:26&#x27;, &#x27;2019-12-15 15:56:26&#x27;,\n",
       "               &#x27;2019-12-27 15:56:25&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lon</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-3ec74e63-0e9e-4d5d-b1fc-d616c64ddcec' class='xr-index-data-in' type='checkbox'/><label for='index-3ec74e63-0e9e-4d5d-b1fc-d616c64ddcec' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-130.7115380813146], dtype=&#x27;float64&#x27;, name=&#x27;lon&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lat</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-049e1367-9d83-4ae6-ba07-9fbca8b1e116' class='xr-index-data-in' type='checkbox'/><label for='index-049e1367-9d83-4ae6-ba07-9fbca8b1e116' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([75.64135503028527], dtype=&#x27;float64&#x27;, name=&#x27;lat&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-03d66a8f-fe6d-41a7-8922-a65d2b062701' class='xr-section-summary-in' type='checkbox'  checked><label for='section-03d66a8f-fe6d-41a7-8922-a65d2b062701' class='xr-section-summary' >Attributes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>crs :</span></dt><dd>EPSG:4326</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 2kB\n",
       "Dimensions:               (time: 79, lon: 1, lat: 1)\n",
       "Coordinates:\n",
       "  * time                  (time) datetime64[ns] 632B 2017-06-04T15:56:08 ... ...\n",
       "  * lon                   (lon) float64 8B -130.7\n",
       "  * lat                   (lat) float64 8B 75.64\n",
       "Data variables:\n",
       "    VH                    (time, lon, lat) float32 316B ...\n",
       "    DEM                   (time, lon, lat) float32 316B ...\n",
       "    rgi_ind_glacier_mask  (time, lon, lat) float32 316B ...\n",
       "Attributes:\n",
       "    crs:      EPSG:4326"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds_fn = \"C:/Users/jaden/Downloads/Research/Notebooks/GEE/testGEE_GRD_merged.nc\"\n",
    "\n",
    "# let xarray try automatically\n",
    "xr.open_dataset(ds_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09af4f1b-e8e6-4180-9bf6-db99e51a09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datacube_to_melt_extent(i, outer_rng, desc, ds_fns, scene_name, rgi_reg, xres, yres, min_glac_area_km2, db_threshold, \n",
    "                                    db_threshold_sl, zscore_threshold, winter_months, snowmelt_months, months2exclude_cp, \n",
    "                                    winter_std_threshold, bin_size, area_bin_size, allmelt_threshold, \n",
    "                                    allmelt_pixels, verbose=True):\n",
    "    # Process Datasets\n",
    "    pathrow_strs = []\n",
    "    failed_glacnos = []\n",
    "    for ds_fn in ds_fns:\n",
    "\n",
    "        print(ds_fn, pol_str)\n",
    "        # Path/Row string used for filenames\n",
    "        pathrow_str = 'P_14_F_387'\n",
    "    \n",
    "        # Initialize the datacube (this is slow because we're loading/opening large datafiles)\n",
    "        dc = sar_datacube(ds_fn, \n",
    "                          scene_name=scene_name,\n",
    "                          rgi_reg=rgi_reg,\n",
    "                          xres=xres,\n",
    "                          yres=yres,\n",
    "                          min_glac_area_km2=min_glac_area_km2,\n",
    "                          db_threshold=db_threshold,\n",
    "                          db_threshold_sl=db_threshold_sl,\n",
    "                          zscore_threshold=zscore_threshold,\n",
    "                          winter_months=winter_months,\n",
    "                          snowmelt_months=snowmelt_months,\n",
    "                          months2exclude_cp=months2exclude_cp,\n",
    "                          winter_std_threshold=winter_std_threshold,\n",
    "                          bin_size=bin_size,\n",
    "                          area_bin_size=area_bin_size,\n",
    "                          allmelt_threshold=allmelt_threshold,\n",
    "                          allmelt_pixels=allmelt_pixels)\n",
    "    \n",
    "        # Load glaciers to process\n",
    "        main_glac_rgi = dc.glacnos_to_process()\n",
    "    \n",
    "        # Add to RGI Path/Row Dictionary\n",
    "        for rgino_str in main_glac_rgi.rgino_str.values:\n",
    "            if not rgino_str in rgino_ds_dict.keys():\n",
    "                rgino_ds_dict[rgino_str] = [ds_fn]\n",
    "            elif not ds_fn in rgino_ds_dict[rgino_str]:\n",
    "                rgino_ds_dict[rgino_str].append(ds_fn)\n",
    "    \n",
    "        # convert PosixPath objects to strings\n",
    "        with open(rgi_ds_fn, 'w') as json_file:\n",
    "            json.dump(rgino_ds_dict, json_file, indent=4)\n",
    "    \n",
    "        # Remove pixels from non-glaciated areas\n",
    "        dc.mask_nonglacier_pixels(main_glac_rgi)\n",
    "    \n",
    "        # PIXEL-BY-PIXEL ANALYSIS\n",
    "        dc.pixel_analysis()\n",
    "        dc.annual_melt_onset_map()\n",
    "    \n",
    "        # SINGLE GLACIER ANALYSIS\n",
    "        for nglac, glacno in enumerate(tqdm(main_glac_rgi.glacno.values, file=desc, \n",
    "                                            desc=f'Processing {len(main_glac_rgi)} glaciers in path_row {pathrow_str} (data_no: {i})')):\n",
    "        # for nglac, glacno in enumerate([5299]): # or specify a particular glacier\n",
    "            outer_rng.set_description(desc.read())\n",
    "            \n",
    "            # RGI_str for filenames\n",
    "            nidx = list(main_glac_rgi.glacno.values).index(glacno)\n",
    "            rgino_str = main_glac_rgi.loc[nidx,'rgino_str']\n",
    "            area_km2 = main_glac_rgi.loc[nidx,'area_km2']\n",
    "    \n",
    "            try:\n",
    "                # Process individual glacier\n",
    "                dc.area_bin_size = area_bin_size\n",
    "                dc.single_glacier_preprocess(glacno=glacno, area_km2=area_km2)\n",
    "    \n",
    "                # Melt Elevation Extents\n",
    "                melt_extent_perc_fn = csv_fp + rgino_str + '_melt_extent_elev_percentile_' + pathrow_str + '.csv'\n",
    "                snowline_perc_fn = csv_fp + rgino_str + '_snowline_elev_percentile_' + pathrow_str + '.csv'\n",
    "                dc.melt_elev_percentile_method(glacno=glacno, csv_fn=melt_extent_perc_fn, csv_sl_fn=snowline_perc_fn)\n",
    "                \n",
    "                # Binned Heatmap: Export CSV file (medium)\n",
    "                db_binned_csv_fn = csv_fp + rgino_str + '_db_bin_mean_' + pathrow_str + '.csv'\n",
    "                hyps_fn = csv_fp + rgino_str + '_hypsometry_' + pathrow_str + '.csv'\n",
    "                dc.db_heatmap(glacno=glacno, csv_fn=db_binned_csv_fn, hyps_fn=hyps_fn)\n",
    "        \n",
    "            except:\n",
    "                failed_glacnos.append(glacno)\n",
    "        \n",
    "        return len(Dataset(ds_fns[0], mode=\"r\").variables[\"time\"]), failed_glacnos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a605e9e-f632-48c7-a14b-594b9ff48aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd2d68d8-a59e-4f47-b45c-8e682a5a6f9c",
   "metadata": {},
   "source": [
    "# PROCESS GLACIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c579559-a71b-43ba-97b7-12a4d139ec47",
   "metadata": {},
   "source": [
    "#### Input information for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22e07339-141c-4365-be57-c20a2007f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_reg = 1            # RGI region (1 for ak)\n",
    "min_glac_area_km2 = 2  # Minimum glacier area to process [sq.km]\n",
    "db_threshold = -3      # Threshold for drop in db relative to winter mean to identify melt\n",
    "db_threshold_sl=4      # Threshold for drop in db relative to summer minimum backscatter to identify snowlines\n",
    "zscore_threshold = -2  # Threshold for z-score to ensure the drop in backscatter exceeds the variance of winter pixels\n",
    "winter_months = [1, 2]  # Months to use to estimate winter months (using Jan/Feb helps avoid rain-on-snow in shoulder seasons)\n",
    "snowmelt_months = [3, 4, 5, 6] # Months to use to estimate snowmelt months for minimum backscatter pixels\n",
    "months2exclude_cp = [10,11,12,1,2] # Month to exclude from change pixel timing\n",
    "winter_std_threshold = 3 # winter month standard deviation threshold [dB] (if a pixel has a larger std than this, it is remove from analysis)\n",
    "#flipped the months bc the southern hemi\n",
    "\n",
    "# recommended bin sizes per type\n",
    "bin_size = 20 # elevation bin sizes in meters\n",
    "area_bin_size = 'variable' # equal-area bin sizes in square meters\n",
    "allmelt_threshold = 0.9 # Threshold above which assume elevations below are melting (deals with clean ice/debris appearing not to melt)\n",
    "allmelt_pixels = 10 # minimum number of pixels to use the allmelt_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa6163-2c8b-46c5-966f-aa5427d587fc",
   "metadata": {},
   "source": [
    "#### Get filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a927be06-e1d4-414b-b836-1d8beaa6116f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location_str': 'Gulkana',\n",
       " 'imagedirsPath': 'D:\\\\glaciers\\\\Gulkana_Raw_SAR',\n",
       " 'scene_name': 'Gulkana',\n",
       " 'epsg_no': 32606,\n",
       " 'path_frame_dict': {'94': ['205']},\n",
       " 'Direction': 'Ascending',\n",
       " 'frame_cut': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a path/frame\n",
    "data_no = str(1)\n",
    "imagedirsPath = loaded_sar_data[data_no]['imagedirsPath']\n",
    "location_str = loaded_sar_data[data_no]['location_str']\n",
    "scene_name = loaded_sar_data[data_no]['scene_name']\n",
    "epsg_no = loaded_sar_data[data_no]['epsg_no']\n",
    "path_frame_dict = loaded_sar_data[data_no]['path_frame_dict']\n",
    "path_direction = loaded_sar_data[data_no]['Direction']\n",
    "frame_cut = loaded_sar_data[data_no]['frame_cut']\n",
    "loaded_sar_data[data_no]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56567bf7-203b-4ea8-88a7-ecca5d0d30db",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8a354d-3de1-46d7-819c-95630b2c4788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/jaden/Downloads/Research/Notebooks/GEE/testGEE_GRD_merged.nc VV\n",
      "Glacnos:\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No glaciers to process",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m#hardcoding dsfns\u001b[39;00m\n\u001b[32m     39\u001b[39m ds_fns = [\u001b[33m\"\u001b[39m\u001b[33mC:/Users/jaden/Downloads/Research/Notebooks/GEE/testGEE_GRD_merged.nc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m scenes_no, failed_glacnos = \u001b[43mprocess_datacube_to_melt_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_no\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[43mouter_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_fns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscene_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscene_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgi_reg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrgi_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxres\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myres\u001b[49m\u001b[43m=\u001b[49m\u001b[43myres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_glac_area_km2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_glac_area_km2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdb_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_threshold_sl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_threshold_sl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzscore_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzscore_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwinter_months\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwinter_months\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43msnowmelt_months\u001b[49m\u001b[43m=\u001b[49m\u001b[43msnowmelt_months\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonths2exclude_cp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmonths2exclude_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwinter_std_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwinter_std_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbin_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marea_bin_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43marea_bin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallmelt_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallmelt_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallmelt_pixels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallmelt_pixels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mscenes:\u001b[39m\u001b[33m'\u001b[39m, scenes_no)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mfailed glaciers:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(failed_glacnos), failed_glacnos, \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mprocess_datacube_to_melt_extent\u001b[39m\u001b[34m(i, outer_rng, desc, ds_fns, scene_name, rgi_reg, xres, yres, min_glac_area_km2, db_threshold, db_threshold_sl, zscore_threshold, winter_months, snowmelt_months, months2exclude_cp, winter_std_threshold, bin_size, area_bin_size, allmelt_threshold, allmelt_pixels, verbose)\u001b[39m\n\u001b[32m     15\u001b[39m dc = sar_datacube(ds_fn, \n\u001b[32m     16\u001b[39m                   scene_name=scene_name,\n\u001b[32m     17\u001b[39m                   rgi_reg=rgi_reg,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m                   allmelt_threshold=allmelt_threshold,\n\u001b[32m     31\u001b[39m                   allmelt_pixels=allmelt_pixels)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Load glaciers to process\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m main_glac_rgi = \u001b[43mdc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglacnos_to_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Add to RGI Path/Row Dictionary\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rgino_str \u001b[38;5;129;01min\u001b[39;00m main_glac_rgi.rgino_str.values:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36msar_datacube.glacnos_to_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(glacnos)\n\u001b[32m    118\u001b[39m glacnos_str = [\u001b[38;5;28mstr\u001b[39m(rgi_reg) + \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(x).zfill(\u001b[32m5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m glacnos]\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(glacnos_str) > \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNo glaciers to process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    121\u001b[39m main_glac_rgi_raw = selectglaciersrgitable(glac_no=glacnos_str, min_glac_area_km2=\u001b[38;5;28mself\u001b[39m.min_glac_area_km2)\n\u001b[32m    122\u001b[39m glacnos_raw = \u001b[38;5;28mlist\u001b[39m(main_glac_rgi_raw.glacno.values)\n",
      "\u001b[31mAssertionError\u001b[39m: No glaciers to process"
     ]
    }
   ],
   "source": [
    "data_nos = [1] # process all data from this path-frame\n",
    "\n",
    "outer_rng = tqdm(data_nos)\n",
    "desc=DescStr()\n",
    "for data_no in outer_rng:\n",
    "    # load data\n",
    "    imagedirsPath = loaded_sar_data[str(data_no)]['imagedirsPath']\n",
    "    location_str = loaded_sar_data[str(data_no)]['location_str']\n",
    "    scene_name = loaded_sar_data[str(data_no)]['scene_name']\n",
    "    epsg_no = loaded_sar_data[str(data_no)]['epsg_no']\n",
    "    path_frame_dict = loaded_sar_data[str(data_no)]['path_frame_dict']\n",
    "    path_direction = loaded_sar_data[str(data_no)]['Direction']\n",
    "    frame_cut = loaded_sar_data[str(data_no)]['frame_cut']\n",
    "    # loaded_sar_data[str(data_no)]\n",
    "\n",
    "    # set up output nc file name start (include epsg code in name)\n",
    "    out_nc_filename_start = f'{location_str}_{epsg_no}_S1_cube'\n",
    "    pol_str = 'VV' # can also be 'HV' depending on your data!\n",
    "    \n",
    "    # dictionary to contain list of images for each date for each path (could be one image per date, or a pair along orbit from that date)\n",
    "    indate_and_zips_dict = {x:[] for x in path_frame_dict.keys()}\n",
    "    ds_fns = []\n",
    "    for path in indate_and_zips_dict.keys():\n",
    "        indate_and_zips = indate_and_zips_dict[path]\n",
    "    \n",
    "        out_nc_filename = str(out_nc) + f'\\{out_nc_filename_start}_{pol_str}_{path}_{\"_\".join(path_frame_dict[path])}.nc'\n",
    "        ds_fns.append(out_nc_filename)\n",
    "\n",
    "    # RGI ID vs path-row dictionary\n",
    "    # makes it easier to access individual runs later on; code simply appends the path/row scenes for each glacier within a dictionary\n",
    "    rgi_ds_fn = 'rgino_ds_dict.json'\n",
    "    if os.path.exists(rgi_ds_fn):\n",
    "        with open(rgi_ds_fn, 'r') as json_file:\n",
    "            rgino_ds_dict = json.load(json_file)\n",
    "    else:\n",
    "        rgino_ds_dict = {}\n",
    "\n",
    "    #hardcoding dsfns\n",
    "    ds_fns = [\"C:/Users/jaden/Downloads/Research/Notebooks/GEE/testGEE_GRD_merged.nc\"]\n",
    "    scenes_no, failed_glacnos = process_datacube_to_melt_extent(i=data_no, outer_rng=outer_rng, desc=desc, ds_fns=ds_fns, \n",
    "            scene_name=scene_name, rgi_reg=rgi_reg, xres=xres, yres=yres, min_glac_area_km2=min_glac_area_km2, \n",
    "            db_threshold=db_threshold, db_threshold_sl=db_threshold_sl, zscore_threshold=zscore_threshold, winter_months=winter_months, \n",
    "            snowmelt_months=snowmelt_months, months2exclude_cp=months2exclude_cp, winter_std_threshold=winter_std_threshold, \n",
    "            bin_size=bin_size, area_bin_size=area_bin_size, allmelt_threshold=allmelt_threshold, allmelt_pixels=allmelt_pixels)\n",
    "    print('scenes:', scenes_no)\n",
    "    print('failed glaciers:', len(failed_glacnos), failed_glacnos, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612a7ad-e1d6-4338-81ed-5d6ed7831414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcbee761-c4a7-4a16-97dd-791d2a25383f",
   "metadata": {},
   "source": [
    "Reorganize files into a folder for each glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5168acf6-856e-40f5-a03b-3cc4b580f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to reorganize csv files into folders\n",
    "import shutil\n",
    "\n",
    "# get all files matching the pattern \"csv01.XXXXX*\"\n",
    "main_directory = os.getcwd()\n",
    "csv_fp = main_directory + '/_csv/'\n",
    "csv_files = glob.glob(os.path.join(csv_fp, f\"{str(rgi_reg).zfill(2)}.[0-9]*.csv\"))\n",
    "\n",
    "\n",
    "# Process each file\n",
    "for csv_file in csv_files:\n",
    "    csv_fn = os.path.basename(csv_file)\n",
    "    glacier_name = csv_fn[:8] # extract glacier name (first 8 characters: \"01.XXXXX\")\n",
    "\n",
    "    glac_folder = os.path.join(csv_fp, glacier_name) # create the target directory if it doesn't exist\n",
    "    os.makedirs(glac_folder, exist_ok=True)\n",
    "\n",
    "    shutil.move(csv_file, os.path.join(glac_folder, csv_fn)) # move the file to its corresponding glacier folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31196c-9881-4481-81f9-28d4b06f40be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec53a9-a199-4e73-9d3b-a6d05da84afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c407e50-45cc-474b-8c46-d48313880325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167171f7-e328-4a4e-8abf-f66d36a898fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b63e1-7818-4386-b99a-11231d996423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840063ff-c317-4235-9b71-6c6d11330a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25ee64-a57d-41c2-99c8-6cdd7c37a4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
